{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimente zur Erstellung eines Deep Learning Modells zur Klassifikation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98683e",
   "metadata": {},
   "source": [
    "### Importieren der benötigten Bibliotheken\n",
    "\n",
    "Hier werden alle notwendigen Bibliotheken und Module importiert, die für die Datenverarbeitung und das Modelltraining benötigt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf \n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_io as tfio\n",
    "import sys\n",
    "sys.path.append(\"../EchoWatch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ae77f",
   "metadata": {},
   "source": [
    "### Herunterladen des Datensatzes\n",
    "\n",
    "Die Pumpendaten werden von einer externen Quelle heruntergeladen und lokal gespeichert. Das Dataset enthält Audiodateien von Maschinengeräuschen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tf.keras.utils.get_file('pump.zip',\n",
    "                        'https://zenodo.org/record/3678171/files/dev_data_pump.zip?download=1',\n",
    "                        cache_dir='./',\n",
    "                        cache_subdir='data',\n",
    "                        extract=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d5133",
   "metadata": {},
   "source": [
    "### Hilfsfunktion zum Laden von WAV-Dateien\n",
    "\n",
    "Die Funktion `load_wav_16k_mono` lädt eine WAV-Datei, konvertiert sie in einen Float-Tensor und resampled sie auf 16 kHz. Dies stellt sicher, dass alle Audiodateien im gleichen Format vorliegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for loading audio files and making sure the sample rate is correct.\n",
    "\n",
    "@tf.function\n",
    "def load_wav_16k_mono(filename):\n",
    "    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n",
    "    file_contents = tf.io.read_file(filename)\n",
    "    wav, sample_rate = tf.audio.decode_wav(\n",
    "          file_contents,\n",
    "          desired_channels=1)\n",
    "    wav = tf.squeeze(wav, axis=-1)\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c7a86",
   "metadata": {},
   "source": [
    "### Definieren der Testdateipfade\n",
    "\n",
    "Hier werden die Pfade für positive (normale) und negative (anomale) Testdateien definiert. Diese Dateien werden später verwendet, um das Audiosignal zu visualisieren und zu analysieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_test_file = os.path.join('data', 'pump', 'test' , 'anomaly_id_00_00000000.wav')\n",
    "pos_test_file = os.path.join('data', 'pump', 'test' , 'normal_id_00_00000000.wav')\n",
    "pos_test_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f37926",
   "metadata": {},
   "source": [
    "### Laden und Visualisieren der WAV-Dateien\n",
    "\n",
    "Die Audiodateien werden geladen und ihre Wellenformen werden visualisiert. Dies gibt einen ersten Einblick in die Struktur der Audiosignale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave = load_wav_16k_mono(pos_test_file)\n",
    "nwave = load_wav_16k_mono(neg_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nwave) # Blau\n",
    "plt.plot(wave) # Orange\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a1490a",
   "metadata": {},
   "source": [
    "### Definieren des Datensatzpfads\n",
    "\n",
    "Der Pfad zum Testdatensatz wird definiert, um den Zugriff auf die Audiodateien zu erleichtern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datensatz = os.path.join('data', 'pump', 'test')\n",
    "Datensatz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae0c497",
   "metadata": {},
   "source": [
    "### Erstellen von Dateipfaden und zugehörigen Labels für den Datensatz\n",
    "\n",
    "Für jede Audiodatei im Datensatz wird der Dateipfad und das zugehörige Label (normal oder anomal) erstellt. Dies erleichtert die weitere Verarbeitung und das Training des Modells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [os.path.join(Datensatz, filename) for filename in os.listdir(Datensatz)]\n",
    "labels = [1 if 'normal' in filename else 0 for filename in os.listdir(Datensatz)]\n",
    "\n",
    "data = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "\n",
    "# Jetzt haben Sie das Dataset mit den Dateipfaden und den entsprechenden Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e2f5c9",
   "metadata": {},
   "source": [
    "### Anzeigen eines zufälligen Eintrags aus dem Datensatz\n",
    "\n",
    "Ein zufälliger Eintrag aus dem Datensatz wird angezeigt, um die Konsistenz und Struktur der Daten zu überprüfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shuffle(10000).as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebb404b",
   "metadata": {},
   "source": [
    "### Anzeigen des gesamten Datensatzes\n",
    "\n",
    "Der gesamte Datensatz wird angezeigt. Dies gibt einen Überblick über die Struktur des Datasets und die vorhandenen Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce39ea67",
   "metadata": {},
   "source": [
    "### Vorverarbeitung der Audiodaten\n",
    "Die Funktion preprocess führt mehrere Schritte zur Vorverarbeitung von Audiodaten durch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path, label): \n",
    "    wav = load_wav_16k_mono(file_path)\n",
    "    wav = wav[:48000]\n",
    "    zero_padding = tf.zeros([48000] - tf.shape(wav), dtype=tf.float32)\n",
    "    wav = tf.concat([zero_padding, wav],0)\n",
    "    spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
    "    return spectrogram, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264b7b7d",
   "metadata": {},
   "source": [
    "### Anpassung und Auffüllen der Audiodaten\n",
    "Das Audiosignal wird zuerst mit load_wav_16k_mono geladen und dann auf eine Länge von 160.000 Samples beschnitten. Falls das Signal kürzer ist, wird es mit Nullen aufgefüllt, um diese Länge zu erreichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav = load_wav_16k_mono(pos_test_file)\n",
    "wav = wav[:160000]\n",
    "zero_padding = tf.zeros([160000] - tf.shape(wav), dtype=tf.float32)\n",
    "wav = tf.concat([zero_padding, wav],0)\n",
    "wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e787ef5",
   "metadata": {},
   "source": [
    "### Erstellung und Anpassung des Spektrogramms\n",
    "Ein Spektrogramm des Audiosignals wird mit der Short-Time Fourier Transform (STFT) erstellt. Anschließend wird der Betrag des Spektrogramms genommen, um komplexe Werte in absolute Werte zu konvertieren. Zuletzt wird eine zusätzliche Dimension hinzugefügt, um das Spektrogramm für nachfolgende Verarbeitungsschritte vorzubereiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)\n",
    "spectrogram = tf.abs(spectrogram)\n",
    "spectrogram = tf.expand_dims(spectrogram, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717033e0",
   "metadata": {},
   "source": [
    "### Zufällige Auswahl von Daten\n",
    "Ein zufälliges Datenpaar, bestehend aus Dateipfad und zugehörigem Label, wird aus dem Datensatz ausgewählt. Der Datensatz wird zuerst durchmischt (shuffle), wobei eine Puffergröße von 10.000 verwendet wird, um die Mischreihenfolge zu bestimmen. Anschließend wird das erste Element des durchmischten Datensatzes ausgewählt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath, label = data.shuffle(buffer_size=10000).as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58a98d7",
   "metadata": {},
   "source": [
    "### Anwendung der Vorverarbeitung\n",
    "Das zuvor ausgewählte Audiosignal (über den Dateipfad) und das zugehörige Label werden durch die preprocess Funktion verarbeitet. Das Ergebnis ist ein Spektrogramm des Audiosignals und das unveränderte Label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram, label = preprocess(filepath, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a24aaa",
   "metadata": {},
   "source": [
    "### Visualisierung des Spektrogramms\n",
    "Das berechnete Spektrogramm wird visualisiert. Die Darstellung erfolgt in einem großformatigen Plot mit den Maßen 30x20. Das Spektrogramm wird transponiert, um es korrekt darzustellen, wobei nur der erste Kanal gezeigt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,20))\n",
    "plt.imshow(tf.transpose(spectrogram)[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db3c162",
   "metadata": {},
   "source": [
    "### Datenverarbeitung und Vorbereitung für das Training\n",
    "Vorverarbeitung: Jedes Element im Datensatz wird mit der preprocess Funktion verarbeitet, um Spektrogramme zu erhalten.\n",
    "\n",
    "Zwischenspeichern: Die Daten werden im Cache gespeichert, um das Laden während des Trainings zu beschleunigen.\n",
    "\n",
    "Mischen: Der Datensatz wird durchmischt, wobei eine Puffergröße von 1.000 verwendet wird.\n",
    "\n",
    "Batching: Die Daten werden in Batches von jeweils 16 Einträgen gruppiert, was für das Training von neuronalen Netzwerken üblich ist.\n",
    "\n",
    "Vorabladen: Es werden 8 Batches im Voraus geladen, um die Datenbereitstellung während des Trainings zu optimieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(preprocess)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=1000)\n",
    "data = data.batch(16)\n",
    "data = data.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69031a9a",
   "metadata": {},
   "source": [
    "### Aufteilen des Datensatzes in Trainings- und Testdaten\n",
    "Der Datensatz wird in zwei Teile aufgeteilt:\n",
    "\n",
    "Trainingsdaten: Die ersten 38 Batches des Datensatzes werden für das Training verwendet. (70%)\n",
    "\n",
    "Testdaten: Die folgenden 16 Batches, die nach Überspringen der ersten 38 Batches kommen, werden als Testdatensatz verwendet. (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.take(38)\n",
    "test = data.skip(38).take(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b969cb1",
   "metadata": {},
   "source": [
    "### Laden des ersten Trainingsbatches\n",
    "Das erste Batch von Trainingsdaten wird geladen. Es enthält sowohl die Spektrogramm-Samples als auch die zugehörigen Labels. Dies kann nützlich sein, um einen Überblick über die Daten zu bekommen oder sie weiter zu analysieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, labels = train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8f4d10",
   "metadata": {},
   "source": [
    "### Erstellen des Modells\n",
    "\n",
    "Ein neuronales Netzwerkmodell wird erstellt, um die Klassifikation durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16, (9,9), activation='relu', input_shape=(1491, 257,1)))\n",
    "model.add(Conv2D(16, (9,9), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d604fc6c",
   "metadata": {},
   "source": [
    "### Erstellen des Modells\n",
    "\n",
    "Ein neuronales Netzwerkmodell wird erstellt, um die Klassifikation durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(16, (6,6), activation='relu', input_shape=(1491, 257, 1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2))) \n",
    "model.add(tf.keras.layers.Conv2D(16, (6,6), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930b4564",
   "metadata": {},
   "source": [
    "### Erstellen des Modells\n",
    "\n",
    "Ein neuronales Netzwerkmodell wird erstellt, um die Klassifikation durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(1491, 257, 1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu'))  # Zusätzliche Conv2D-Schicht\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D())  # Ersetzt Flatten\n",
    "model.add(tf.keras.layers.Dropout(0.5))  # Hilft gegen Overfitting\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))  # Reduzierte Neuronenanzahl\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2b3471",
   "metadata": {},
   "source": [
    "### Erstellen des Modells\n",
    "\n",
    "Ein neuronales Netzwerkmodell wird erstellt, um die Klassifikation durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "    \n",
    "# Erste Conv2D Schicht\n",
    "model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(1491, 257, 1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "# Zweite Conv2D Schicht\n",
    "model.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "# Dritte Conv2D Schicht\n",
    "model.add(tf.keras.layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "# Dense Schichten\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3393f6eb",
   "metadata": {},
   "source": [
    "### Erstellen des Modells\n",
    "\n",
    "Ein neuronales Netzwerkmodell wird erstellt, um die Klassifikation durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "    \n",
    "# Erste Conv2D Schicht\n",
    "model.add(tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(1491, 257, 1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    " \n",
    "# Zweite Conv2D Schicht\n",
    "model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    # Dritte Conv2D Schicht\n",
    "model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    # Dense Schichten\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7838b7e",
   "metadata": {},
   "source": [
    "### Erstellen des Modells\n",
    "\n",
    "Ein neuronales Netzwerkmodell wird erstellt, um die Klassifikation durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "# Erste Conv2D Schicht\n",
    "model.add(tf.keras.layers.Conv2D(8, (3,3), activation='relu', input_shape=(1491, 257, 1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2, strides=2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    # Zweite Conv2D Schicht\n",
    "model.add(tf.keras.layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2, strides=2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    # Dritte Conv2D Schicht\n",
    "model.add(tf.keras.layers.Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2, strides=2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "# Dense Schichten\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88218635",
   "metadata": {},
   "source": [
    "### Erstellen des Modells\n",
    "\n",
    "Ein neuronales Netzwerkmodell wird erstellt, um die Klassifikation durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    # Input Layer: The input shape is derived from your spectrogram shape\n",
    "    # First Convolutional Layer\n",
    "    Conv2D(16, (3, 3), activation='relu', input_shape=(spectrogram.shape[1], spectrogram.shape[2], 1)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    # Second Convolutional Layer\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    # Third Convolutional Layer\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    # Flatten Layer\n",
    "    Flatten(),\n",
    "\n",
    "    # Fully Connected Layer\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "\n",
    "    # Output Layer\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53520eb3",
   "metadata": {},
   "source": [
    "### Erstellen des Modells\n",
    "\n",
    "Ein neuronales Netzwerkmodell wird erstellt, um die Klassifikation durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Convolutional layer 1\n",
    "model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(1491, 257, 1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "# Convolutional layer 2\n",
    "model.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "# Convolutional layer 3\n",
    "model.add(tf.keras.layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "# Flatten and dense layers\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69098111",
   "metadata": {},
   "source": [
    "### Erstellen des Modells\n",
    "\n",
    "Ein neuronales Netzwerkmodell wird erstellt, um die Klassifikation durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_more_compact_cnn_model(input_shape):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Convolutional layer 1\n",
    "    model.add(tf.keras.layers.Conv2D(8, (3,3), activation='relu', strides=2, input_shape=input_shape))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    # Convolutional layer 2\n",
    "    model.add(tf.keras.layers.Conv2D(16, (3,3), activation='relu', strides=2))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    # Convolutional layer 3\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu', strides=2))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(2))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    # Flatten and dense layers\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "    model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='Adam', \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy', tf.keras.metrics.Recall(), tf.keras.metrics.Precision()])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bbdf87",
   "metadata": {},
   "source": [
    "### Kompilieren des Modells\n",
    "\n",
    "Das Modell wird mit einem Optimierer, einem Verlust und Metriken kompiliert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('Adam', loss='BinaryCrossentropy', metrics=['accuracy', tf.keras.metrics.Recall(), tf.keras.metrics.Precision()]) #mit accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3896dc4b",
   "metadata": {},
   "source": [
    "### Anzeigen des Modells inklusive Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f9b87",
   "metadata": {},
   "source": [
    "### Training des Modells\n",
    "\n",
    "Das Modell wird mit den Trainingsdaten trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train, epochs=20, validation_data=test) #Pro Epoche ca. 2min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf756efa",
   "metadata": {},
   "source": [
    "### Visualisierung der Daten\n",
    "\n",
    "Die Daten werden visualisiert, um Trends, Muster oder Anomalien zu erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(hist.history['loss'], 'r')\n",
    "plt.plot(hist.history['val_loss'], 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Precision')\n",
    "plt.plot(hist.history['precision_14'], 'r')\n",
    "plt.plot(hist.history['val_precision_14'], 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Recall')\n",
    "plt.plot(hist.history['recall_14'], 'r')\n",
    "plt.plot(hist.history['val_recall_14'], 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('accuracy')\n",
    "plt.plot(hist.history['accuracy'], 'r')\n",
    "plt.plot(hist.history['val_accuracy'], 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadc1980",
   "metadata": {},
   "source": [
    "### Bewertung des Modells\n",
    "\n",
    "Das trainierte Modell wird mit den Testdaten bewertet, um seine Leistung zu messen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy, test_recall, test_precision = model.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb2413d",
   "metadata": {},
   "source": [
    "### Anzeigen der Testergebnisse\n",
    "Die Ergebnisse des Modelltests, einschließlich Verlust, Genauigkeit, Recall und Precision, werden ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Recall: {test_recall}\")\n",
    "print(f\"Test Precision: {test_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a51cc53",
   "metadata": {},
   "source": [
    "### Evaluation mit den Trainingsdaten\n",
    "Das Modell wird mit den Trainingsdaten bewertet, um den Verlust und die Genauigkeit zu ermitteln. Diese Metriken geben Aufschluss darüber, wie gut das Modell auf den Daten trainiert wurde, mit denen es auch trainiert wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(train)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bfa4a5",
   "metadata": {},
   "source": [
    "### Vorbereitung der Testdaten\n",
    "Das erste Batch von Testdaten wird geladen, um sowohl die Eingabedaten (X_test) als auch die zugehörigen Labels (y_test) zu erhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = test.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa1acd7",
   "metadata": {},
   "source": [
    "### Vorhersage mit dem Modell\n",
    "Das trainierte Modell wird verwendet, um Vorhersagen für die Testdaten zu treffen. Die Vorhersagen sind kontinuierliche Werte, die anschließend in binäre Werte (1 oder 0) umgewandelt werden, basierend auf einem Schwellenwert von 0,5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = [1 if prediction > 0.5 else 0 for prediction in yhat]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2858e19",
   "metadata": {},
   "source": [
    "### Konvertierung der Testlabels\n",
    "Die Testlabels (y_test) werden in den Integer-Datentyp konvertiert, um sie mit den binären Vorhersagen vergleichen zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
