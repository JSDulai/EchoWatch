{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "K2madPFAGHb3"
   },
   "source": [
    "# Transfer Learning mit YAMNet zur Klassifizierung von Pump Geräuschen\n",
    "\n",
    "[YAMNet](https://tfhub.dev/google/yamnet/1) ist ein vortrainiertes tiefes neuronales Netzwerk, das Audio-Ereignisse aus [521 Klassen](https://github.com/tensorflow/models/blob/master/research/audioset/yamnet/yamnet_class_map.csv) vorhersagen kann, wie z.B. Lachen, Bellen oder eine Sirene.\n",
    "\n",
    " In diesem Tutorial werden Sie lernen, wie man:\n",
    "\n",
    "- Laden und Verwenden des YAMNet-Modells für die Inferenz.\n",
    "- Erstellen eines neuen Modells unter Verwendung der YAMNet-Einbettungen zur Klassifizierung von 'normal' und 'anomaly' Pumpen-Geräuschen.\n",
    "- Ihr Modell zu evaluieren und zu exportieren."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5Mdp2TpBh96Y"
   },
   "source": [
    "## TensorFlow und andere Bibliotheken importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7l3nqdWVF-kC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import zipfile\n",
    "import csv\n",
    "import random\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_io as tfio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "v9ZhybCnt_bM"
   },
   "source": [
    "## Über YAMNet\n",
    "\n",
    "[YAMNet](https://github.com/tensorflow/models/tree/master/research/audioset/yamnet) ist ein vortrainiertes neuronales Netzwerk, das die tiefenselektierbare Faltungsarchitektur von [MobileNetV1](https://arxiv.org/abs/1704.04861) verwendet. Es kann eine Audiowellenform als Eingabe verwenden und unabhängige Vorhersagen für jedes der 521 Audioereignisse aus dem [AudioSet](http://g.co/audioset) Korpus machen.\n",
    "\n",
    "Intern extrahiert das Modell \"Rahmen\" aus dem Audiosignal und verarbeitet Stapel dieser Rahmen. Diese Version des Modells verwendet Rahmen mit einer Länge von 0,96 Sekunden und extrahiert alle 0,48 Sekunden einen Rahmen.\n",
    "\n",
    "Das Modell akzeptiert ein 1-D float32 Tensor oder NumPy Array, das eine Wellenform beliebiger Länge enthält, dargestellt als einkanalige (mono) 16 kHz Samples im Bereich `[-1.0, +1.0]`. Dieses Tutorial enthält Code, mit dem Sie WAV-Dateien in das unterstützte Format konvertieren können.\n",
    "\n",
    "Das Modell liefert 3 Ergebnisse, darunter die Klassenwerte, die Einbettungen (die Sie für das Transferlernen verwenden werden) und das log mel [Spektrogramm] (https://www.tensorflow.org/tutorials/audio/simple_audio#spectrogram). Weitere Einzelheiten finden Sie [hier](https://tfhub.dev/google/yamnet/1).\n",
    "\n",
    "Eine spezifische Verwendung von YAMNet ist als High-Level-Feature-Extraktor - die 1.024-dimensionale Einbettungsausgabe. Sie werden die Eingangsmerkmale des Basismodells (YAMNet) verwenden und sie in Ihr flacheres Modell einspeisen, das aus einer versteckten Schicht \"tf.keras.layers.Dense\" besteht. Dann trainieren Sie das Netzwerk auf einer kleinen Datenmenge für die Audioklassifizierung, ohne dass Sie eine große Menge an beschrifteten Daten und ein End-to-End-Training benötigen. (Dies ist vergleichbar mit [transfer learning for image classification with TensorFlow Hub] (https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub) für weitere Informationen).\n",
    "\n",
    "Zuerst werden Sie das Modell testen und die Ergebnisse der Klassifizierung von Audio sehen. Sie werden dann die Datenvorverarbeitungspipeline aufbauen.\n",
    "\n",
    "### Laden von YAMNet vom TensorFlow Hub\n",
    "\n",
    "Sie werden ein vortrainiertes YAMNet von [Tensorflow Hub] (https://tfhub.dev/) verwenden, um die Einbettungen aus den Audiodateien zu extrahieren.\n",
    "\n",
    "Das Laden eines Modells von TensorFlow Hub ist einfach: wählen Sie das Modell, kopieren Sie seine URL und benutzen Sie die Funktion `load`.\n",
    "\n",
    "Hinweis: Um die Dokumentation des Modells zu lesen, verwenden Sie die Modell-URL in Ihrem Browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06CWkBV5v3gr"
   },
   "outputs": [],
   "source": [
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GmrPJ0GHw9rr"
   },
   "source": [
    "Wenn das Modell geladen ist, können Sie dem [YAMNet basic usage tutorial] (https://www.tensorflow.org/hub/tutorials/yamnet) folgen und eine WAV-Beispieldatei herunterladen, um die Inferenz durchzuführen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mBm9y9iV2U_-"
   },
   "source": [
    "Sie benötigen eine Funktion zum Laden von Audiodateien, die auch später bei der Arbeit mit den Trainingsdaten verwendet wird. (Erfahren Sie mehr über das Lesen von Audiodateien und deren Labels in [Einfache Audioerkennung](https://www.tensorflow.org/tutorials/audio/simple_audio#reading_audio_files_and_their_labels).\n",
    "\n",
    "Hinweis: Die von `load_wav_16k_mono` zurückgegebenen `wav_data` sind bereits auf Werte im Bereich `[-1.0, 1.0]` normalisiert (weitere Informationen finden Sie in [YAMNet's documentation on TF Hub](https://tfhub.dev/google/yamnet/1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xwc9Wrdg2EtY"
   },
   "outputs": [],
   "source": [
    "# Utility functions for loading audio files and making sure the sample rate is correct.\n",
    "\n",
    "@tf.function\n",
    "def load_wav_16k_mono(filename):\n",
    "    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n",
    "    file_contents = tf.io.read_file(filename)\n",
    "    wav, sample_rate = tf.audio.decode_wav(\n",
    "          file_contents,\n",
    "          desired_channels=1)\n",
    "    wav = tf.squeeze(wav, axis=-1)\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmthELBg1A2-"
   },
   "source": [
    "## MIMII dataset - Pump dataset\n",
    "\n",
    "Der MIMII-Datensatz ist eine Sammlung von Geräuschaufnahmen aus der Industrie, die normale und anomale Geräusche von Maschinen wie Ventilen und Pumpen umfasst. Die Aufnahmen wurden mit einer Acht-Kanal-Mikrofonanordnung in realen Fabrikumgebungen erstellt, um eine realistische Darstellung von Industriegeräuschen zu bieten. Dieser Daten-satz dient als Benchmark für die akustische Fehlerdiagnose von Maschinen. (Purohit et al., 2019)\n",
    "\n",
    "Datensatz herunterladen und extrahieren!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWobqK8JmZOU"
   },
   "outputs": [],
   "source": [
    "_ = tf.keras.utils.get_file('dev_data_pump.zip',\n",
    "                        'https://zenodo.org/record/3678171/files/dev_data_pump.zip?download=1',\n",
    "                        cache_dir='./',\n",
    "                        cache_subdir='datasets',\n",
    "                        extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_wav_data = load_wav_16k_mono('./datasets/pump/test/normal_id_00_00000000.wav')\n",
    "\n",
    "_ = plt.plot(testing_wav_data)\n",
    "\n",
    "# Play the audio file.\n",
    "display.Audio(testing_wav_data, rate=16000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qcruxiuX1cO5"
   },
   "source": [
    "### Erkunden Sie die Daten\n",
    "\n",
    "Die Metadaten für jede Datei sind in der csv-Datei, welche hier vorher noch erstellt werden muss und anschließen in `./datasets/pump/pump.csv` gespeichert wird.\n",
    "\n",
    "und alle Audiodateien befinden sich in `./datasets/pump/test`\n",
    "\n",
    "Ein Pandas `DataFrame` mit Mapping wird erstellt um einen besseren Überblick über die Daten zu erhalten.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird vorher noch eine csv Datei erstellt.\n",
    "Es wird auch noch eine fold Spalte hinzugefügt für das Cross Validation Verfahren, wo alle Daten in gleichgroße Teilmengen aufgeteilt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = './datasets/pump/test'\n",
    "num_folds = 5\n",
    "\n",
    "file_data = []\n",
    "\n",
    "#Durchsuche das Verzeichnis und sammle Dateinamen und Klassen\n",
    "for filename in os.listdir(audio_dir):\n",
    "    if filename.endswith('.wav'):  #Hier wird angenommen, die Audiodateien haben das Format .wav\n",
    "        if 'normal' in filename:\n",
    "            file_data.append((filename, 'normal'))\n",
    "        elif 'anomaly' in filename:\n",
    "            file_data.append((filename, 'anomaly'))\n",
    "\n",
    "random.shuffle(file_data)\n",
    "\n",
    "fold_size = len(file_data) // num_folds # Größe der Foldteilmenge wird berechnet\n",
    "\n",
    "# Die Folds werden erstellt\n",
    "folds = []\n",
    "for i in range(num_folds):\n",
    "    fold_start = i * fold_size\n",
    "    fold_end = (i + 1) * fold_size\n",
    "    fold_data = file_data[fold_start:fold_end]\n",
    "    folds.append(fold_data)\n",
    "\n",
    "# Pfad zur CSV-Datei\n",
    "csv_file = './datasets/pump/pump.csv'\n",
    "\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['filename', 'category', 'fold']) \n",
    "\n",
    "    # Daten werden in die CSV Datei geschrieben\n",
    "    for fold_index, fold_data in enumerate(folds):\n",
    "        for filename, label in fold_data:\n",
    "            writer.writerow([filename, label, fold_index + 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwmLygPrMAbH"
   },
   "outputs": [],
   "source": [
    "pump_csv = './datasets/pump/pump.csv'\n",
    "base_data_path = './datasets/pump/test/'\n",
    "\n",
    "pd_data = pd.read_csv(pump_csv)\n",
    "pd_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7d4rHBEQ2QAU"
   },
   "source": [
    "### Filtern Sie die Daten\n",
    "\n",
    "Jetzt, wo die Daten im `DataFrame` gespeichert sind, wenden Sie einige Transformationen an:\n",
    "\n",
    "- Filtern Sie die Zeilen heraus und verwenden Sie nur die ausgewählten Klassen - \"normal\" und \"anomal\".\n",
    "- Ändern Sie den Dateinamen so, dass er den vollständigen Pfad enthält. Dies wird das Laden später erleichtern.\n",
    "- Ändern Sie die Ziele so, dass sie innerhalb eines bestimmten Bereichs liegen. In diesem Beispiel bleibt `normal` auf `0`, aber `anomaly` wird zu `1`.\n",
    "\n",
    "Übersetzt mit www.DeepL.com/Translator (kostenlose Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFnEoQjgs14I"
   },
   "outputs": [],
   "source": [
    "my_classes = ['normal', 'anomaly']\n",
    "map_class_to_id = {'normal':0, 'anomaly':1}\n",
    "\n",
    "filtered_pd = pd_data[pd_data.category.isin(my_classes)]  #sucht in der Spalte \"category\" speziell nach dem Inhalt der Liste my_classes also [normal, anomaly]\n",
    "\n",
    "class_id = filtered_pd['category'].apply(lambda name: map_class_to_id[name])\n",
    "filtered_pd = filtered_pd.assign(target=class_id)\n",
    "\n",
    "full_path = filtered_pd['filename'].apply(lambda row: os.path.join(base_data_path, row))\n",
    "filtered_pd = filtered_pd.assign(filename=full_path)\n",
    "\n",
    "filtered_pd.head(10)\n",
    "print(filtered_pd.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BkDcBS-aJdCz"
   },
   "source": [
    "### Laden der Audiodateien und Abrufen von Einbettungen\n",
    "\n",
    "Hier wenden Sie die Funktion `load_wav_16k_mono` an und bereiten die WAV-Daten für das Modell vor.\n",
    "\n",
    "Wenn Sie Einbettungen aus den WAV-Daten extrahieren, erhalten Sie ein Array der Form `(N, 1024)`, wobei `N` die Anzahl der Frames ist, die YAMNet gefunden hat (einer für jede 0,48 Sekunden Audio)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AKDT5RomaDKO"
   },
   "source": [
    "Ihr Modell wird jeden Rahmen als eine Eingabe verwenden. Daher müssen Sie eine neue Spalte erstellen, die einen Rahmen pro Zeile enthält. Außerdem müssen Sie die Beschriftungen und die Spalte \"fold\" erweitern, um diese neuen Zeilen korrekt wiederzugeben.\n",
    "\n",
    "Die erweiterte Spalte \"fold\" behält die ursprünglichen Werte bei. Sie können keine Frames mischen, da bei der Durchführung der Splits Teile desselben Audios in verschiedenen Splits auftauchen könnten, was Ihre Validierungs- und Testschritte weniger effektiv machen würde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5Rq3_PyKLtU"
   },
   "outputs": [],
   "source": [
    "filenames = filtered_pd['filename']\n",
    "targets = filtered_pd['target']\n",
    "folds = filtered_pd['fold']\n",
    "\n",
    "main_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsEfovDVAHGY"
   },
   "outputs": [],
   "source": [
    "def load_wav_for_map(filename, label, fold):\n",
    "  return load_wav_16k_mono(filename), label, fold\n",
    "\n",
    "main_ds = main_ds.map(load_wav_for_map)\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0tG8DBNAHcE"
   },
   "outputs": [],
   "source": [
    "# applies the embedding extraction model to a wav data\n",
    "def extract_embedding(wav_data, label, fold):\n",
    "  ''' run YAMNet to extract embedding from the wav data '''\n",
    "  scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
    "  num_embeddings = tf.shape(embeddings)[0]\n",
    "  return (embeddings,\n",
    "            tf.repeat(label, num_embeddings),\n",
    "            tf.repeat(fold, num_embeddings))\n",
    "\n",
    "# extract embedding\n",
    "main_ds = main_ds.map(extract_embedding).unbatch()\n",
    "main_ds.element_spec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdfPIeD0Qedk"
   },
   "source": [
    "### Aufteilung des Datensatzes\n",
    "\n",
    "Verwenden Sie die Spalte \"fold\", um den Datensatz in Trainings-, Validierungs- und Testdatensätze aufzuteilen.\n",
    "\n",
    "Der letzte Schritt besteht darin, die Spalte \"fold\" aus dem Datensatz zu entfernen, da Sie sie beim Training nicht verwenden werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ZYvlFiVsffC"
   },
   "outputs": [],
   "source": [
    "cached_ds = main_ds.cache()\n",
    "train_ds = cached_ds.filter(lambda embedding, label, fold: fold < 4)\n",
    "val_ds = cached_ds.filter(lambda embedding, label, fold: fold == 4)\n",
    "test_ds = cached_ds.filter(lambda embedding, label, fold: fold == 5)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
    "\n",
    "train_ds = train_ds.map(remove_fold_column)\n",
    "val_ds = val_ds.map(remove_fold_column)\n",
    "test_ds = test_ds.map(remove_fold_column)\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "v5PaMwvtcAIe"
   },
   "source": [
    "## Erstellen Sie Ihr Modell\n",
    "\n",
    "Definieren Sie als nächstes ein sehr einfaches [Sequential](https://www.tensorflow.org/guide/keras/sequential_model) Modell mit einer versteckten Schicht und zwei Ausgängen, um 'normal' und 'anomly Geräusche zu erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYCE0Fr1GpN3"
   },
   "outputs": [],
   "source": [
    "my_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(my_classes))\n",
    "], name='my_model')\n",
    "\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1qgH35HY0SE"
   },
   "outputs": [],
   "source": [
    "my_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                            patience=3,\n",
    "                                            restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3sj84eOZ3pk"
   },
   "outputs": [],
   "source": [
    "history = my_model.fit(train_ds,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_ds,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OAbraYKYpdoE"
   },
   "source": [
    "Führen wir die Methode `evaluate` an den Testdaten durch, um sicherzugehen, dass es keine Überanpassung gibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H4Nh5nec3Sky"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = my_model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "cid-qIrIpqHS"
   },
   "source": [
    "Sie haben es geschafft!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nCKZonrJcXab"
   },
   "source": [
    "## Testen Sie Ihr Modell\n",
    "\n",
    "Als Nächstes testen Sie Ihr Modell mit der Einbettung aus dem vorherigen Test, indem Sie nur YAMNet verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79AFpA3_ctCF"
   },
   "outputs": [],
   "source": [
    "scores, embeddings, spectrogram = yamnet_model(testing_wav_data)\n",
    "result = my_model(embeddings).numpy()\n",
    "\n",
    "inferred_class = my_classes[result.mean(axis=0).argmax()]\n",
    "print(f'The main sound is: {inferred_class}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "k2yleeev645r"
   },
   "source": [
    "## Speichern Sie ein Modell, das direkt eine WAV-Datei als Eingabe verwenden kann.\n",
    "\n",
    "Ihr Modell funktioniert, wenn Sie ihm die Einbettungen als Eingabe geben.\n",
    "\n",
    "In einem realen Szenario werden Sie Audiodaten als direkte Eingabe verwenden wollen.\n",
    "\n",
    "Zu diesem Zweck kombinieren Sie YAMNet mit Ihrem Modell zu einem einzigen Modell, das Sie für andere Anwendungen exportieren können.\n",
    "\n",
    "Um die Verwendung der Ergebnisse des Modells zu erleichtern, wird die letzte Schicht eine `reduce_mean` Operation sein. Wenn Sie dieses Modell zum Servieren verwenden (was Sie später im Lehrgang lernen werden), benötigen Sie den Namen der letzten Ebene. Wenn Sie keinen definieren, wird TensorFlow automatisch einen inkrementellen Namen definieren, was das Testen erschwert, da er sich jedes Mal ändert, wenn Sie das Modell trainieren. Wenn Sie eine rohe TensorFlow Operation verwenden, können Sie ihr keinen Namen zuweisen. Um dieses Problem zu lösen, erstellen Sie einen benutzerdefinierten Layer, der `reduce_mean` anwendet und ihn `classifier` nennt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUVCI2Suunpw"
   },
   "outputs": [],
   "source": [
    "class ReduceMeanLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, axis=0, **kwargs):\n",
    "    super(ReduceMeanLayer, self).__init__(**kwargs)\n",
    "    self.axis = axis\n",
    "\n",
    "  def call(self, input):\n",
    "    return tf.math.reduce_mean(input, axis=self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zE_Npm0nzlwc"
   },
   "outputs": [],
   "source": [
    "saved_model_path = './pump_yamnet'\n",
    "\n",
    "input_segment = tf.keras.layers.Input(shape=(), dtype=tf.float32, name='audio')\n",
    "embedding_extraction_layer = hub.KerasLayer(yamnet_model_handle,\n",
    "                                            trainable=False, name='yamnet')\n",
    "_, embeddings_output, _ = embedding_extraction_layer(input_segment)\n",
    "serving_outputs = my_model(embeddings_output)\n",
    "serving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)\n",
    "serving_model = tf.keras.Model(input_segment, serving_outputs)\n",
    "serving_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-0bY5FMme1C"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(serving_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "btHQDN9mqxM_"
   },
   "source": [
    "Laden Sie Ihr gespeichertes Modell, um zu überprüfen, ob es wie erwartet funktioniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkYVpJS72WWB"
   },
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4BkmvvNzq49l"
   },
   "source": [
    "Und für den letzten Test: Liefert Ihr Modell bei einigen soliden Daten das richtige Ergebnis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xeXtD5HO28y-"
   },
   "outputs": [],
   "source": [
    "reloaded_results = reloaded_model(testing_wav_data)\n",
    "normal_or_anomal = my_classes[tf.math.argmax(reloaded_results)]\n",
    "print(f'The main sound is: {normal_or_anomal}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRrOcBYTUgwn"
   },
   "source": [
    "Wenn Sie Ihr neues Modell in einem Serving-Setup ausprobieren möchten, können Sie die Signatur \"serving_default\" verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycC8zzDSUG2s"
   },
   "outputs": [],
   "source": [
    "serving_results = reloaded_model.signatures['serving_default'](testing_wav_data)\n",
    "cat_or_dog = my_classes[tf.math.argmax(serving_results['classifier'])]\n",
    "print(f'The main sound is: {normal_or_anomal}')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "transfer_learning_audio.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
